## 计量经济学 Econometrics

初级计量经济学的一切都是围绕 线性回归与最小二乘法 这个事情展开的。他在 Quant 中确实应用频繁，掌握基本的性质很有必要。

### 1. 核心思想
最小二乘法（Least Squares Method）的核心思想是通过**最小化残差平方和**（Sum of Squared Residuals）来估计模型参数，从而找到数据的最佳函数匹配。其数学本质是优化问题，目标是使模型预测值与实际观测值之间的误差平方和最小。  

设观测数据点为 \((x_i, y_i)\)，模型预测值为 \(\hat{y}_i = f(x_i, \beta)\)，其中 \(\beta\) 为待估参数。目标函数为：$\min_{\beta} S = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \left( y_i - f(x_i, \beta) \right)^2$
通过求解此优化问题，得到参数 \(\beta\) 的最优估计。  

### 2. 数学原理与推导
*   **一元线性回归模型**：通过求导法求解斜率与截距的解析解。

    * 一元线性回归模型是回归分析中最基础的形式，用于描述一个自变量与一个因变量之间的线性关系[1,2,3](@ref)。其标准数学表达式为：
    $Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$ 其中：

    *   $Y_i$ 是第 $i$ 个观测值的**因变量**（Dependent Variable），也称为被解释变量或响应变量[1,4](@ref)。

    *   $X_i$ 是第 $i$ 个观测值的**自变量**（Independent Variable），也称为解释变量或预测变量[1,4](@ref)。

    *   $\beta_0$ 是**截距项**（Intercept），表示当自变量 $X=0$ 时，因变量 $Y$ 的基准预期值[4](@ref)。

    *   $\beta_1$ 是**斜率**（Slope），即回归系数，表示自变量 $X$ 每变动一个单位，因变量 $Y$ 平均变动的量[4](@ref)。

    *   $\varepsilon_i$ 是**随机误差项**（Random Error Term），包含了所有未观测因素对 $Y_i$ 的影响，其均值为零。

*   **多元线性回归模型**：使用矩阵运算推导参数估计量，其公式为 $\beta = (X'X)^{-1}X'y$。这里的 $X$ 是解释变量数据矩阵，$y$ 是被解释变量向量[1](@ref)。

    * 当因变量的变化受到多个重要因素的影响时，需要使用多元线性回归模型[6,7,8](@ref)。其一般形式为：
    $Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_k X_{ki} + \varepsilon_i$ 其中：

    *   $X_{1i}, X_{2i}, \ldots, X_{ki}$ 是 $k$ 个自变量在第 $i$ 个观测值上的取值[6,7](@ref)。

    *   $\beta_1, \beta_2, \ldots, \beta_k$ 为**偏回归系数**（Partial Regression Coefficients）[6,7](@ref)。$\beta_j$ 表示在控制其他自变量不变的情况下，$X_j$ 每增加一个单位，对 
    $Y$ 产生的平均效应[6,7,8](@ref)。

    *   $\beta_0$ 是**截距项**，表示所有自变量取值为0时因变量的预期水平（在实际情况中，该值可能没有直接的实际意义）[6](@ref)。

    *   $\varepsilon_i$ 同样是**随机误差项**，包含了所有未纳入模型的随机因素对 $Y_i$ 的影响[$citation:6]。


### 3. 基本假设
OLS估计的有效性依赖于一系列**经典假设**：

1.  **模型设定正确**：模型是线性的，且没有设定误差。

2.  **解释变量非随机或与误差项无关**：解释变量是固定的，或者即使是随机的，也与误差项无关（$\operatorname{cov}(X, \varepsilon) = 0$）。

3.  **误差项的条件均值为零**：$E(\varepsilon \mid X) = 0$。

4.  **误差项同方差**：所有误差项的方差相等，$\operatorname{Var}(\varepsilon \mid X) = \sigma^2$。

5.  **误差项无自相关**：不同观测值的误差项之间不相关，$\operatorname{Cov}(\varepsilon_i, \varepsilon_j \mid X) = 0, i \neq j$。

6.  **解释变量之间无完全多重共线性**：数据矩阵 $X$ 是满秩的。

### 4. 统计性质（高斯-马尔可夫定理）
在满足经典假设的条件下，OLS估计量具有以下优良的统计性质，被称为**最优线性无偏估计量（BLUE）**：

*   **线性性**：估计量 $\hat{\beta}$ 是 $y$ 的线性组合。

*   **无偏性**：估计量的期望值等于总体参数真值，即 $E(\hat{\beta}) = \beta$。

*   **有效性**：在所有线性无偏估计量中，OLS估计量的方差最小。

### 5. 估计方法：OLS的变体与实践
当经典假设不满足时，直接使用OLS可能会得到有偏或无效的估计结果，因此发展出多种改进的估计方法：

*   **加权最小二乘法（WLS）**：当误差项存在**异方差性**（Heteroskedasticity）时，即误差项的方差随观测值变化，WLS通过给不同观测值赋予不同的权重来修正异方差，使变换后的模型满足同方差假设。其基本思想是对原模型加权，使之变成一个新的不存在异方差性的模型，然后采用OLS估计其参数。

*   **广义最小二乘法（GLS）**：是WLS的更一般形式，用于处理**已知结构的异方差**或**序列相关**（Autocorrelation）问题。OLS可视为GLS在权重矩阵为单位矩阵时的特例。

*   **工具变量法（IV）** & **两阶段最小二乘法（2SLS）**：当解释变量与误差项**相关**（存在内生性问题）时，OLS估计是有偏的。工具变量法通过寻找一个与内生解释变量相关但与误差项不相关的工具变量来解决。2SLS是IV的一种应用，适用于恰好识别和过度识别的模型：第一阶段用内生变量对工具变量回归得到预测值；第二阶段用被解释变量对这个预测值进行回归。

### 6. 模型检验
估计参数后，需要进行一系列检验来评估模型质量和变量显著性：

*   **拟合优度检验**：

    *   **判定系数（$R^2$）**：衡量模型对因变量变动的解释程度，$R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$。其值越接近1，说明模型拟合度越好[1](@ref)。

    *   **调整后的判定系数（Adjusted $R^2$）**：考虑到增加解释变量会人为提高$R^2$，调整$R^2$引入了自由度惩罚，能更客观地评价模型优劣。

*   **变量显著性检验（t检验）**：检验某个解释变量是否对被解释变量有显著影响。原假设通常是该变量的系数为0。

*   **方程显著性检验（F检验）**：检验所有解释变量的系数是否**联合显著**不为零，即模型整体是否有效。

*   **残差检验**：用于检验模型是否满足经典假设，例如：

    *   **异方差检验**：如怀特检验（White Test）、BP检验。

    *   **自相关检验**：如杜宾-沃森检验（D-W Test）。